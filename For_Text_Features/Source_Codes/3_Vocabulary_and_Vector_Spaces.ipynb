{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary : Bag of Words\n",
    "Suppose we have a list of tweets from the internet and we want to perform sentiment analysis on these tweets - train a model that classifies between postive and negative sentiment tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tweets = [\"The movie was amazing.\", \"I liked the actor's costume.\",.....,\"I was not a fan of the climax.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the model of choice for this classification task is Logistic Regression which takes in a fixed-length vector that represents a tweet and classifies this tweet. For this NLP task, tweets/sentences written in English need to be converted into numbers. How do we do that? We build a vocabulary of words that contains all the words that exist in this training set of tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vocabulary**: A set of words that contains all the words in the training set (minus the ones removed from stemming and lemmatization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Binary Vector Representation of a sentence\n",
    "Once a a vocabulary is defined, each tweet can be represented by a vector equal to the length of the vocabulary. Each position in the vector represents each word in the vocabulary, usually in the alphabetical order.<br>\n",
    "For each tweet, if a certain word from the vocabulary is present in the tweet, this position in the feature vector of this tweet is assigned 1, else 0. As such, each tweet now can be represented as a feature vector with 0's and 1's, of length equal to the length of the vocabulary. This representation can be used with algorithms likes Logistic Regression and Bernoulli Naive Bayes model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Frequency Vector Representation of a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This representation is identical to the binary vector representation of a sentence, except that each element in the vector represents the total frequency of the corresponding word in the sentence rather than whether this word was present or not in the sentence. As such, the length of the representation is still |V|. However, this representation contains more information than the binary representation of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem with a binary and frquency vector representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating feature vectors with the length equal to the length of the vocabulary, |V|, means each tweet has |V| features. In practice, each tweet is very short in comparison to the size of the vocabulary, thus resulting in feature vectors that have negligible amount of 1's and almost all 0's. Since these feature vectors are intended to be fed to algorithms liks Logistic Regression or Naive Bayes, larger the number of features of each sample, the more number of parameters that these algorithms will need to learn, which is not desired in machine learning for reasons beyond the scope of this notebook (like overfitting.) Further, the training time as well as the inference time increase with the number of parameters that need to be learned.<br><br>\n",
    "It would make more sense to have a different representation scheme that does not require |V| dimensions to represent a tweet. We will see those schemes next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Feature Representation with Postive/Negative Frequency  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, our task at hand is sentiment classification. For this task, we want to represent a sentence mathematically in such a way that this representation does not lose the information that it contains either positive or negative sentiments. Following is one way to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of looking at individual words of a tweet in its vector representation, each tweet can instead be represented using just 2 dimensions (3, if you count the bias term if used with an algorithm that uses a bias term). For this, for each word in the vocabulary, first count the frequency that each word appears in the postive and the negative tweets. After that, Just add the total frequencies for the words in the tweet in each of the classes separately i.e., each tweet can be represented as<br>\n",
    "tweet = [$\\Sigma$freq(words of this tweet in +ve class), $\\Sigma$freq(words of this tweet in -ve class)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing that, we are essentially preserving the positive or negative sentiment that a sentence carries while significantly reducing the representation to just 2 dimensions from |V|. This brings in huge advantage in terms of training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.1 Building Word Frequenciese Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be observed in the Sentiment-Analysis repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.2 Visualizing Word Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be observed in the Sentiment-Analysis repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Varying-Length Vector Representation \n",
    "All the above representations have a predefined length. As such, they fit well with an algorithm that always expects a fixed length input such as Logistic Regression. However, a varying-length representation can also be used to represent a sentence. However, this would not be feasible for a Logistic Regression model but instead for a model that is agnostic about the length of the input. For example, Multinomial Naive Bayes model (Please follow Stanford slides)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
