{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the text data is cleaned of impurities, it is ready for tokenization, lemmatization, and stemming.<br>\n",
    "Here, however, we will token a raw text directly for the sake of illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenization is the process of splitting paragraphs into sentences or sentences into individual words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\", quiet = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path_to_file_1 = \"../data/example_text.txt\"\n",
    "with open(path_to_file_1) as f:\n",
    "    raw_text_1 = f.read()        # reads all the contents into a single string\n",
    "                             # can read one line at a time and treat each line as separate using readlines() \n",
    "\n",
    "# Read Reviews       \n",
    "def build_data_frame(path):\n",
    "    rows = []\n",
    "    index = []\n",
    "    review_count = 0\n",
    "    \n",
    "    all_files = os.listdir(path)#os.walk(path)\n",
    "    \n",
    "    for filename in all_files:\n",
    "        \n",
    "        with open(path+filename) as f:\n",
    "            raw_text = f.read()\n",
    "            rows.append(raw_text)\n",
    "        index.append(filename)\n",
    "        review_count += 1\n",
    "        \n",
    "        # We will use only 300 review for simplicity for now.\n",
    "        if review_count == 300:\n",
    "            break\n",
    "    \n",
    "\n",
    "    data_frame = pd.DataFrame(rows, index=index)\n",
    "    return data_frame\n",
    "\n",
    "        \n",
    "path_to_files_2 = \"../data/imdb/train/unsup/\"\n",
    "\n",
    "reviews = build_data_frame(path_to_files_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I admit, the great majority of films released before say 1933 are just not for me.',\n",
       " 'Of the dozen or so \"major\" silents I have viewed, one I loved (The Crowd), and two were very good (The Last Command and City Lights, that latter Chaplin circa 1931).<br /><br />So I was apprehensive about this one, and humor is often difficult to appreciate (uh, enjoy) decades later.',\n",
       " 'I did like the lead actors, but thought little of the film.<br /><br />One intriguing sequence.',\n",
       " 'Early on, the guys are supposed to get \"de-loused\" and for about three minutes, fully dressed, do some schtick.',\n",
       " 'In the background, perhaps three dozen men pass by, all naked, white and black (WWI ?',\n",
       " '), and for most, their butts, part or full backside, are shown.',\n",
       " 'Was this an early variation of beefcake courtesy of Howard Hughes?']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize sentences\n",
    "\n",
    "# Method 1\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(reviews.iloc[0,0])\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I admit, the great majority of films released before say 1933 are just not for me.',\n",
       " 'Of the dozen or so \"major\" silents I have viewed, one I loved (The Crowd), and two were very good (The Last Command and City Lights, that latter Chaplin circa 1931).<br /><br />So I was apprehensive about this one, and humor is often difficult to appreciate (uh, enjoy) decades later.',\n",
       " 'I did like the lead actors, but thought little of the film.<br /><br />One intriguing sequence.',\n",
       " 'Early on, the guys are supposed to get \"de-loused\" and for about three minutes, fully dressed, do some schtick.',\n",
       " 'In the background, perhaps three dozen men pass by, all naked, white and black (WWI ?',\n",
       " '), and for most, their butts, part or full backside, are shown.',\n",
       " 'Was this an early variation of beefcake courtesy of Howard Hughes?']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 2\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "sentences = tokenizer.tokenize(reviews.iloc[0,0])\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dancing',\n",
       " 'is',\n",
       " 'what',\n",
       " 'I',\n",
       " 'do',\n",
       " '.',\n",
       " 'I',\n",
       " 'came',\n",
       " 'riding',\n",
       " 'on',\n",
       " 'a',\n",
       " 'bike',\n",
       " '.',\n",
       " 'He',\n",
       " 'eats',\n",
       " 'pasta',\n",
       " '.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"Dancing is what I do. I came riding on a bike. He eats pasta.\"\n",
    "\n",
    "# Method 1\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(sample_text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dancing',\n",
       " 'is',\n",
       " 'what',\n",
       " 'I',\n",
       " 'do.',\n",
       " 'I',\n",
       " 'came',\n",
       " 'riding',\n",
       " 'on',\n",
       " 'a',\n",
       " 'bike.',\n",
       " 'He',\n",
       " 'eats',\n",
       " 'pasta',\n",
       " '.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 2\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer_obj = TreebankWordTokenizer()\n",
    "\n",
    "words = tokenizer_obj.tokenize(sample_text)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the methods above show that if apostrophes are not handled before passing to the nltk tokenizer, we get undesired results.<br>\n",
    "nltk offers another tokenizer that can use regular expression matching to avoid such undesired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dancing',\n",
       " 'is',\n",
       " 'what',\n",
       " 'I',\n",
       " 'do',\n",
       " 'I',\n",
       " 'came',\n",
       " 'riding',\n",
       " 'on',\n",
       " 'a',\n",
       " 'bike',\n",
       " 'He',\n",
       " 'eats',\n",
       " 'pasta']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "words = regexp_tokenize(sample_text, \"[\\w']+\")\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dancing',\n",
       " 'is',\n",
       " 'what',\n",
       " 'I',\n",
       " 'do',\n",
       " 'I',\n",
       " 'came',\n",
       " 'riding',\n",
       " 'on',\n",
       " 'a',\n",
       " 'bike',\n",
       " 'He',\n",
       " 'eats',\n",
       " 'pasta']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 2\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "words = tokenizer.tokenize(sample_text)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "In language, the same word can have different forms depending upon its usage. For example, eat  has different forms such as ate, eaten, eating, and eats. Because of these different form, there is redundancy in thr corpus. We could mean the same thing with the use of different forms. For example, I rode from school to my house. VS I came riding from school to my house. It is obvious that \"rode\" and \"riding\" were conveying the same information. We want to remove this redundancy from the data and this process is called stemming. Stemming literally means \"to remove stems from a fruit/ vegetable\".<br><br>\n",
    "Typically, a stemmer only truncates a word from the end.<br>\n",
    "nltk library has multiple stemmer available: PorterStemmer, LancasterStemmer, RegexpStemmer, SnowballStemmer<br>\n",
    "PorterStemmer is the least aggressive among all the  nltk stemmer. LancasterStemmer is very aggressive. RegexpStemmer allows us to pass a regular expression to stem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['danc',\n",
       " 'is',\n",
       " 'what',\n",
       " 'I',\n",
       " 'do',\n",
       " 'I',\n",
       " 'came',\n",
       " 'ride',\n",
       " 'on',\n",
       " 'a',\n",
       " 'bike',\n",
       " 'He',\n",
       " 'eat',\n",
       " 'pasta']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "pstemmer = PorterStemmer()\n",
    "stemmed_words = [pstemmer.stem(word) for word in words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dant',\n",
       " 'is',\n",
       " 'what',\n",
       " 'i',\n",
       " 'do',\n",
       " 'i',\n",
       " 'cam',\n",
       " 'rid',\n",
       " 'on',\n",
       " 'a',\n",
       " 'bik',\n",
       " 'he',\n",
       " 'eat',\n",
       " 'past']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "lstemmer = LancasterStemmer()\n",
    "stemmed_words = [lstemmer.stem(word) for word in words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While stemming appears to give us the root word (called lemma), it can lead to wrong lemmas because most of the times it is only truncating the word without any linguistic analysis/ consideration. For example, stemming \"caring\" using a RegexStemmer to  filter \"ing\" gives us \"Car\" which has an entirely different meaning than the actual lemma \"care\". Hence, instead of stemming, we should resort to lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Lemma : \"run\" is lemma for \"running\". \"ride\" is lemma for \"rides\" or \"riding\".\n",
    "Lemmatization is the process of converting words into their roots/ lemmas such that the meaning does not get distorted as seen above for \"caring\". Also, stemming might produce words that do not exist in the language but lemmatization always produces a meaningful word. For such correct conversion, the PartOf Speech (POS) of the word should be knowm i.e., noun/ verb/adjective, etc.<br>\n",
    "For example, if we lemmatize the word \"meeting\" based on what POS it belongs to i.e., noun, we get \"meeting\" as the lemma. However, if we pass it through a regular stemmer, it returns \"meet\" which could mean a verb or a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"cacti\", \"gods\", \"rains\", \"ate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cactus', 'god', 'rain', 'ate']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we mentioned above that a lemmatizer needs to know what POS a particular word belongs to. By default, WordNetLemmatizer takes each word as a noun. That is why the lemmatizer was unable to change \"ate\" to \"eat\" as it considered \"ate\" a noun and no lemma was found in its library.<br>\n",
    "TO make sure correct lemmatization happens, we can do as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"ate\", pos = \"v\") # where v means verb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
